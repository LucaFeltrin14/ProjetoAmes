{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto: Previsão de Preços de Imóveis em Ames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das Bibliotecas Necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de Dados\n",
    "import pathlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pré-processamento e Modelagem\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Configurações Gerais\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento e Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho para o diretório de dados\n",
    "DATA_DIR = pathlib.Path.cwd().parent / 'data'\n",
    "\n",
    "# Construir o caminho completo para o arquivo\n",
    "data_path = DATA_DIR / 'processed' / 'ames_with_correct_types.pkl'\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if data_path.exists():\n",
    "    print(\"Dados encontrados. Carregando o arquivo...\")\n",
    "    with open(data_path, 'rb') as file:\n",
    "        data, continuous_variables, discrete_variables, ordinal_variables, categorical_variables = pickle.load(file)\n",
    "    print(\"Dados carregados com sucesso.\")\n",
    "else:\n",
    "    print(\"Erro: O arquivo não foi encontrado. Verifique o caminho e tente novamente.\")\n",
    "\n",
    "# Criar uma cópia do dataframe para manipulação\n",
    "df = data.copy()\n",
    "\n",
    "# Visualizar as primeiras linhas do dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Exploratória de Dados (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informações Gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações sobre o dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a porcentagem de valores ausentes em cada coluna\n",
    "missing_percent = df.isnull().mean() * 100\n",
    "\n",
    "# Exibir as colunas com valores ausentes\n",
    "missing_percent[missing_percent > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização de Distribuições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a distribuição de 'SalePrice'\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df['SalePrice'], kde=True)\n",
    "plt.title('Distribuição de SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlação das variáveis numéricas\n",
    "numeric_cols = continuous_variables + discrete_variables\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Heatmap da matriz de correlação\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title('Matriz de Correlação')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engenharia de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratamento de Valores Ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover Colunas com Muitos Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas com mais de 50% de valores ausentes\n",
    "cols_to_drop = missing_percent[missing_percent > 50].index.tolist()\n",
    "print(f'Colunas a serem removidas devido a muitos valores ausentes: {cols_to_drop}')\n",
    "\n",
    "# Remover as colunas\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputação de Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputação para variáveis numéricas\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in num_cols:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Imputação para variáveis categóricas\n",
    "cat_cols = df.select_dtypes(include=['category']).columns\n",
    "for col in cat_cols:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].cat.add_categories(['Missing'])\n",
    "        df[col] = df[col].fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformação da Variável Alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de SalePrice antes da transformação\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df['SalePrice'], kde=True)\n",
    "plt.title('Distribuição de SalePrice (Antes da Transformação)')\n",
    "plt.xlabel('SalePrice')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()\n",
    "\n",
    "# Aplicar logaritmo natural\n",
    "df['SalePrice'] = np.log(df['SalePrice'])\n",
    "\n",
    "# Histograma de SalePrice após a transformação\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df['SalePrice'], kde=True, color='green')\n",
    "plt.title('Distribuição de SalePrice (Após a Transformação Logarítmica)')\n",
    "plt.xlabel('Log(SalePrice)')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação de Novas Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total de Banheiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma nova feature somando todos os banheiros\n",
    "df['TotalBathrooms'] = (\n",
    "    df['Bsmt.Full.Bath'] + (0.5 * df['Bsmt.Half.Bath']) +\n",
    "    df['Full.Bath'] + (0.5 * df['Half.Bath'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idade da Casa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a idade da casa no ano da venda\n",
    "df['HouseAge'] = df['Yr.Sold'] - df['Year.Built']\n",
    "df['HouseAge'] = df['HouseAge'].apply(lambda x: x if x >= 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Área Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somar áreas relevantes\n",
    "df['TotalSF'] = df['Total.Bsmt.SF'] + df['X1st.Flr.SF'] + df['X2nd.Flr.SF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão dos Dados em Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e target\n",
    "X = df.drop(columns='SalePrice')\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Dividir os dados em treinamento e teste (75% treino, 25% teste)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificação de Variáveis Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identificar colunas categóricas e numéricas\n",
    "categorical_cols = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Inicializar o OneHotEncoder com o parâmetro atualizado\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Ajustar o encoder nos dados de treinamento e transformar os dados de treinamento e teste\n",
    "X_train_cat = ohe.fit_transform(X_train[categorical_cols])\n",
    "X_test_cat = ohe.transform(X_test[categorical_cols])\n",
    "\n",
    "# Obter os nomes das novas colunas\n",
    "ohe_columns = ohe.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Converter para DataFrame e manter o índice original\n",
    "X_train_cat = pd.DataFrame(X_train_cat, columns=ohe_columns, index=X_train.index)\n",
    "X_test_cat = pd.DataFrame(X_test_cat, columns=ohe_columns, index=X_test.index)\n",
    "\n",
    "# Concatenar as features numéricas e categóricas\n",
    "X_train = pd.concat([X_train[numerical_cols].reset_index(drop=True), X_train_cat.reset_index(drop=True)], axis=1)\n",
    "X_test = pd.concat([X_test[numerical_cols].reset_index(drop=True), X_test_cat.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalonamento das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Inicializar o escalonador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar o escalonador nos dados de treinamento\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "\n",
    "# Transformar os dados de teste\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção e Treinamento de Múltiplos Modelos de Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Linear Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o modelo\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Treinar o modelo\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Prever no conjunto de teste\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Calcular o RMSE\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "print(f'RMSE Regressão Linear: {rmse_lr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a grade de hiperparâmetros\n",
    "ridge_params = {'alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "# Instanciar o modelo\n",
    "ridge = Ridge()\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge, ridge_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "# Melhor modelo e hiperparâmetros\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "print(f'Melhor alpha para Ridge: {ridge_grid.best_params_['alpha']}')\n",
    "\n",
    "# Previsões e RMSE\n",
    "y_pred_ridge = best_ridge.predict(X_test)\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "print(f'RMSE Regressão Ridge: {rmse_ridge:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a grade de hiperparâmetros\n",
    "lasso_params = {'alpha': np.logspace(-3, 1, 5)}\n",
    "\n",
    "# Instanciar o modelo\n",
    "lasso = Lasso(max_iter=10000)\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "lasso_grid = GridSearchCV(\n",
    "    lasso, lasso_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "\n",
    "# Melhor modelo e hiperparâmetros\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "print(f'Melhor alpha para Lasso: {lasso_grid.best_params_['alpha']}')\n",
    "\n",
    "# Previsões e RMSE\n",
    "y_pred_lasso = best_lasso.predict(X_test)\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "print(f'RMSE Regressão Lasso: {rmse_lasso:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a grade de hiperparâmetros\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Instanciar o modelo\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "rf_grid = GridSearchCV(\n",
    "    rf, rf_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Melhor modelo e hiperparâmetros\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(f'Melhores hiperparâmetros para Random Forest: {rf_grid.best_params_}')\n",
    "\n",
    "# Previsões e RMSE\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(f'RMSE Random Forest: {rmse_rf:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação dos Desempenhos dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame com os resultados\n",
    "results = pd.DataFrame({\n",
    "    'Modelo': ['Regressão Linear', 'Regressão Ridge', 'Regressão Lasso', 'Random Forest'],\n",
    "    'RMSE': [rmse_lr, rmse_ridge, rmse_lasso, rmse_rf]\n",
    "})\n",
    "\n",
    "# Ordenar por RMSE\n",
    "results = results.sort_values('RMSE').reset_index(drop=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='RMSE', y='Modelo', data=results, palette='viridis')\n",
    "plt.title('Comparação dos Modelos')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Modelo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo Selecionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas de Desempenho no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
