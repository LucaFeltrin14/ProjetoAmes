{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class AmesDataProcessor:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = pathlib.Path(data_dir)\n",
    "        pd.set_option('display.max_rows', 500)\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the initial processed data from pickle file.\"\"\"\n",
    "        processed_file_path = self.data_dir / 'processed' / 'ames_with_correct_types.pkl'\n",
    "        with open(processed_file_path, 'rb') as file:\n",
    "            (self.data,\n",
    "             self.continuous_variables,\n",
    "             self.discrete_variables,\n",
    "             self.ordinal_variables,\n",
    "             self.categorical_variables) = pickle.load(file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remap_categories(series: pd.Series, old_categories: tuple[str], new_category: str) -> pd.Series:\n",
    "        \"\"\"Remap multiple categories to a single new category.\"\"\"\n",
    "        series = series.cat.add_categories(new_category)\n",
    "        remapped_items = series.isin(old_categories)\n",
    "        series.loc[remapped_items] = new_category\n",
    "        return series.cat.remove_unused_categories()\n",
    "    \n",
    "    def process_zoning(self):\n",
    "        \"\"\"Process MS.Zoning column.\"\"\"\n",
    "        selection = ~(self.data['MS.Zoning'].isin(['A (agr)', 'C (all)', 'I (all)']))\n",
    "        self.data = self.data[selection]\n",
    "        self.data['MS.Zoning'] = self.data['MS.Zoning'].cat.remove_unused_categories()\n",
    "    \n",
    "    def process_sale_type(self):\n",
    "        \"\"\"Process Sale.Type column.\"\"\"\n",
    "        self.data['Sale.Type'] = self.remap_categories(\n",
    "            series=self.data['Sale.Type'],\n",
    "            old_categories=('WD ', 'CWD', 'VWD'),\n",
    "            new_category='GroupedWD'\n",
    "        )\n",
    "        self.data['Sale.Type'] = self.remap_categories(\n",
    "            series=self.data['Sale.Type'],\n",
    "            old_categories=('COD', 'ConLI', 'Con', 'ConLD', 'Oth', 'ConLw'),\n",
    "            new_category='Other'\n",
    "        )\n",
    "    \n",
    "    def process_conditions(self):\n",
    "        \"\"\"Process Condition columns.\"\"\"\n",
    "        for col in ('Condition.1', 'Condition.2'):\n",
    "            self.data[col] = self.remap_categories(\n",
    "                series=self.data[col],\n",
    "                old_categories=('RRAn', 'RRAe', 'RRNn', 'RRNe'),\n",
    "                new_category='Railroad'\n",
    "            )\n",
    "            self.data[col] = self.remap_categories(\n",
    "                series=self.data[col],\n",
    "                old_categories=('Feedr', 'Artery'),\n",
    "                new_category='Roads'\n",
    "            )\n",
    "            self.data[col] = self.remap_categories(\n",
    "                series=self.data[col],\n",
    "                old_categories=('PosA', 'PosN'),\n",
    "                new_category='Positive'\n",
    "            )\n",
    "        \n",
    "        self.create_combined_condition()\n",
    "        self.data = self.data.drop(columns=['Condition.1', 'Condition.2'])\n",
    "    \n",
    "    def create_combined_condition(self):\n",
    "        \"\"\"Create combined Condition column from Condition.1 and Condition.2.\"\"\"\n",
    "        self.data['Condition'] = pd.Series(\n",
    "            index=self.data.index,\n",
    "            dtype=pd.CategoricalDtype(categories=(\n",
    "                'Norm', 'Railroad', 'Roads', 'Positive', 'RoadsAndRailroad'\n",
    "            ))\n",
    "        )\n",
    "        \n",
    "        # Set conditions based on rules\n",
    "        norm_items = self.data['Condition.1'] == 'Norm'\n",
    "        self.data['Condition'][norm_items] = 'Norm'\n",
    "        \n",
    "        railroad_items = (self.data['Condition.1'] == 'Railroad') & (self.data['Condition.2'] == 'Norm')\n",
    "        self.data['Condition'][railroad_items] = 'Railroad'\n",
    "        \n",
    "        roads_items = (self.data['Condition.1'] == 'Roads') & (self.data['Condition.2'] != 'Railroad')\n",
    "        self.data['Condition'][roads_items] = 'Roads'\n",
    "        \n",
    "        positive_items = self.data['Condition.1'] == 'Positive'\n",
    "        self.data['Condition'][positive_items] = 'Positive'\n",
    "        \n",
    "        roads_and_railroad_items = (\n",
    "            (self.data['Condition.1'] == 'Railroad') & (self.data['Condition.2'] == 'Roads')\n",
    "        ) | (\n",
    "            (self.data['Condition.1'] == 'Roads') & (self.data['Condition.2'] == 'Railroad')\n",
    "        )\n",
    "        self.data['Condition'][roads_and_railroad_items] = 'RoadsAndRailroad'\n",
    "    \n",
    "    def process_features(self):\n",
    "        \"\"\"Process miscellaneous features.\"\"\"\n",
    "        # Create HasShed feature\n",
    "        self.data['HasShed'] = self.data['Misc.Feature'] == 'Shed'\n",
    "        self.data = self.data.drop(columns='Misc.Feature')\n",
    "        \n",
    "        # Create HasAlley feature\n",
    "        self.data['HasAlley'] = ~self.data['Alley'].isna()\n",
    "        self.data = self.data.drop(columns='Alley')\n",
    "    \n",
    "    def process_exterior(self):\n",
    "        \"\"\"Process exterior-related columns.\"\"\"\n",
    "        # Fix inconsistencies in Exterior.2nd\n",
    "        self.data['Exterior.2nd'] = self.remap_categories(\n",
    "            series=self.data['Exterior.2nd'],\n",
    "            old_categories=('Brk Cmn',),\n",
    "            new_category='BrkComm'\n",
    "        )\n",
    "        self.data['Exterior.2nd'] = self.remap_categories(\n",
    "            series=self.data['Exterior.2nd'],\n",
    "            old_categories=('CmentBd',),\n",
    "            new_category='CemntBd'\n",
    "        )\n",
    "        self.data['Exterior.2nd'] = self.remap_categories(\n",
    "            series=self.data['Exterior.2nd'],\n",
    "            old_categories=('Wd Shng',),\n",
    "            new_category='WdShing'\n",
    "        )\n",
    "        \n",
    "        # Sort categories\n",
    "        for col in ('Exterior.1st', 'Exterior.2nd'):\n",
    "            categories = self.data[col].cat.categories\n",
    "            self.data[col] = self.data[col].cat.reorder_categories(sorted(categories))\n",
    "        \n",
    "        # Group rare materials\n",
    "        mat_count = self.data['Exterior.1st'].value_counts()\n",
    "        rare_materials = list(mat_count[mat_count < 40].index)\n",
    "        self.data['Exterior'] = self.remap_categories(\n",
    "            series=self.data['Exterior.1st'],\n",
    "            old_categories=rare_materials,\n",
    "            new_category='Other'\n",
    "        )\n",
    "        self.data = self.data.drop(columns=['Exterior.1st', 'Exterior.2nd'])\n",
    "    \n",
    "    def process_numerical_features(self):\n",
    "        \"\"\"Process numerical features.\"\"\"\n",
    "        # Transform SalePrice\n",
    "        self.data['SalePrice'] = self.data['SalePrice'].apply(np.log10)\n",
    "        \n",
    "        # Handle Lot.Frontage\n",
    "        self.data['Lot.Frontage'] = self.data['Lot.Frontage'].fillna(self.data['Lot.Frontage'].median())\n",
    "        \n",
    "        # Process garage age\n",
    "        self.process_garage_age()\n",
    "        \n",
    "        # Process house and remodeling age\n",
    "        self.process_house_ages()\n",
    "        \n",
    "        # Handle masonry veneer area\n",
    "        self.data.loc[self.data['Mas.Vnr.Area'].isna(), 'Mas.Vnr.Area'] = 0.0\n",
    "    \n",
    "    def process_garage_age(self):\n",
    "        \"\"\"Process garage age-related features.\"\"\"\n",
    "        garage_age = self.data['Yr.Sold'] - self.data['Garage.Yr.Blt']\n",
    "        garage_age[garage_age < 0.0] = 0.0\n",
    "        self.data = self.data.drop(columns='Garage.Yr.Blt')\n",
    "        self.data['Garage.Age'] = garage_age\n",
    "        self.data['Garage.Age'] = self.data['Garage.Age'].fillna(self.data['Garage.Age'].median())\n",
    "    \n",
    "    def process_house_ages(self):\n",
    "        \"\"\"Process house age-related features.\"\"\"\n",
    "        remod_age = self.data['Yr.Sold'] - self.data['Year.Remod.Add']\n",
    "        remod_age[remod_age < 0.0] = 0.0\n",
    "        \n",
    "        house_age = self.data['Yr.Sold'] - self.data['Year.Built']\n",
    "        house_age[house_age < 0.0] = 0.0\n",
    "        \n",
    "        self.data = self.data.drop(columns=['Year.Remod.Add', 'Year.Built'])\n",
    "        self.data['Remod.Age'] = remod_age\n",
    "        self.data['House.Age'] = house_age\n",
    "    \n",
    "    def clean_data(self):\n",
    "        \"\"\"Perform final cleaning steps.\"\"\"\n",
    "        # Drop unnecessary columns\n",
    "        columns_to_drop = ['Street', 'Utilities', 'Pool.QC', 'Fireplace.Qu',\n",
    "                          'Garage.Cond', 'Garage.Qual', 'Heating']\n",
    "        self.data = self.data.drop(columns=columns_to_drop)\n",
    "        \n",
    "        # Handle missing values\n",
    "        self.data = self.data.dropna(axis=0)\n",
    "        \n",
    "        # Clean up categories\n",
    "        for col in self.data.select_dtypes('category').columns:\n",
    "            self.data[col] = self.data[col].cat.remove_unused_categories()\n",
    "    \n",
    "    def save_clean_data(self):\n",
    "        \"\"\"Save the cleaned data to a pickle file.\"\"\"\n",
    "        clean_data_path = self.data_dir / 'processed' / 'ames_clean.pkl'\n",
    "        with open(clean_data_path, 'wb') as file:\n",
    "            pickle.dump(self.data, file)\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"Execute all data processing steps.\"\"\"\n",
    "        self.load_data()\n",
    "        self.process_zoning()\n",
    "        self.process_sale_type()\n",
    "        self.process_conditions()\n",
    "        self.process_features()\n",
    "        self.process_exterior()\n",
    "        self.process_numerical_features()\n",
    "        self.clean_data()\n",
    "        self.save_clean_data()\n",
    "        return self.data\n",
    "\n",
    "def main(path):\n",
    "    data_dir = path\n",
    "    processor = AmesDataProcessor(data_dir)\n",
    "    processed_data = processor.process_data()\n",
    "    print(\"Data processing completed successfully.\")\n",
    "    return processed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Scikit-learn - Preprocessamento\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    PolynomialFeatures,\n",
    "    OneHotEncoder\n",
    ")\n",
    "\n",
    "# Scikit-learn - Modelos\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Scikit-learn - Pipeline e Composição\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Scikit-learn - Divisão de dados e Validação\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "# Scikit-learn - Métricas\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Função que carrega os dados de um arquivo CSV usando uma função customizada chamada `main`\n",
    "# (Assume-se que a função `main()` já esteja definida em outro ponto do código)\n",
    "data = main(pathlib.Path.cwd().parent / 'data')\n",
    "\n",
    "# Realiza a codificação das variáveis categóricas com a técnica One-Hot Encoding\n",
    "# O parâmetro `drop_first=True` é utilizado para evitar a multicolinearidade,\n",
    "# removendo a primeira coluna codificada (efeito \"dummy variable trap\")\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Converter todas as colunas que possuem dados booleanos para o tipo float\n",
    "# Isso é necessário porque alguns modelos esperam que os dados estejam em formato numérico\n",
    "data = data.astype({col: 'float' for col in data.select_dtypes(include=['bool']).columns})\n",
    "\n",
    "# Separar o DataFrame em duas partes:\n",
    "# `X` contém todas as colunas que representam as features (variáveis independentes),\n",
    "# enquanto `y` é a coluna alvo (variável dependente) que estamos tentando prever, neste caso 'SalePrice'\n",
    "X = data.drop(columns='SalePrice')  # Remover a coluna 'SalePrice' para criar as features\n",
    "y = data['SalePrice']  # Selecionar 'SalePrice' como a variável alvo\n",
    "\n",
    "# Até esse ponto:\n",
    "# - O conjunto de dados foi carregado e preparado.\n",
    "# - Todas as variáveis categóricas foram convertidas em variáveis numéricas utilizando One-Hot Encoding.\n",
    "# - As colunas booleanas foram convertidas para o tipo float para garantir compatibilidade com os modelos.\n",
    "# - `X` e `y` representam, respectivamente, as features e o target para modelagem preditiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'model__alpha': 100.0, 'poly__degree': 1}, -0.0030260112118220064)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divisão dos dados em conjuntos de treino e teste\n",
    "# X: Features (variáveis independentes)\n",
    "# y: Target (variável dependente)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                  # Matrix de features\n",
    "    y,                  # Vetor target\n",
    "    test_size=0.2,      # 20% dos dados para teste\n",
    "    random_state=42     # Semente para reprodutibilidade\n",
    ")\n",
    "\n",
    "# Criação do pipeline de preprocessamento e modelo\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),     # Padronização das features (média 0, variância 1)\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),  # Geração de features polinomiais\n",
    "    ('model', Ridge())                # Modelo de regressão Ridge\n",
    "])\n",
    "\n",
    "# Grade de hiperparâmetros para busca\n",
    "Ridge_param_grid = {\n",
    "    'poly__degree': [1, 2, 3],        # Graus polinomiais a serem testados\n",
    "    'model__alpha': np.logspace(-3, 3, 7)  # Valores de regularização (10^-3 a 10^3)\n",
    "}\n",
    "\n",
    "# Configuração da busca em grade com validação cruzada\n",
    "Ridge_grid_search = GridSearchCV(\n",
    "    ridge_pipeline,         # Pipeline a ser otimizado\n",
    "    Ridge_param_grid,       # Grade de parâmetros\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Métrica de avaliação\n",
    "    n_jobs=-1              # Usa todas as CPUs disponíveis\n",
    ")\n",
    "\n",
    "# Treina o modelo com todos os parâmetros da grade\n",
    "Ridge_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retorna os melhores parâmetros e score\n",
    "best_params = Ridge_grid_search.best_params_  # Melhores hiperparâmetros encontrados\n",
    "best_score = Ridge_grid_search.best_score_    # Melhor score na validação cruzada\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'model__max_depth': 10, 'model__n_estimators': 100}, -0.0035533534493707316)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Criação do pipeline com Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('model', RandomForestRegressor(n_jobs=-1))  # n_jobs=-1 utiliza todas as CPUs disponíveis\n",
    "])\n",
    "\n",
    "# Define a grade de hiperparâmetros para otimização\n",
    "rf_param_grid = {\n",
    "    'model__n_estimators': [50, 100],  # Número de árvores na floresta\n",
    "    'model__max_depth': [5, 10]        # Profundidade máxima de cada árvore\n",
    "}\n",
    "\n",
    "# Configuração da busca em grade com validação cruzada\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipeline,           # Pipeline a ser otimizado\n",
    "    rf_param_grid,         # Grade de parâmetros\n",
    "    cv=5,                  # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Métrica de avaliação\n",
    "    n_jobs=-1             # Usa todas as CPUs disponíveis\n",
    ")\n",
    "\n",
    "# Treina o modelo com todos os parâmetros da grade\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Obtém os melhores parâmetros e score\n",
    "best_params = rf_grid.best_params_    # Melhores hiperparâmetros encontrados\n",
    "best_score = rf_grid.best_score_      # Melhor score na validação cruzada\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model (Ridge) test RMSE: 0.0432\n",
      "Erro percentual: 10.46%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Obtém o melhor modelo da busca em grade\n",
    "best_model = Ridge_grid_search.best_estimator_  # Extrai o modelo com melhores parâmetros\n",
    "best_model_name = \"Ridge\"                      # Nome do modelo para referência\n",
    "\n",
    "# Treina o melhor modelo com os dados de treino\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Faz previsões no conjunto de teste\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Calcula o RMSE (Root Mean Squared Error)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "# Imprime o resultado do RMSE\n",
    "print(f\"Best model ({best_model_name}) test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Calcula o erro percentual médio\n",
    "# Converte RMSE para escala original (desfaz a transformação logarítmica)\n",
    "erro = 10**test_rmse - 1  # Subtrai 1 para obter a proporção do erro\n",
    "erro_percentual = erro * 100  # Converte para percentual\n",
    "print(\"Erro percentual: {:.2f}%\".format(erro_percentual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores da coluna SalePrice estão em escala logarítmica de base 10. Portanto, para calcular o erro em termos absolutos, é necessário aplicar a função inversa, ou seja, 10^x. O mesmo se aplica ao preço das casas: por exemplo, um valor de 5 corresponde a um preço real de 10^5. O RMSE (erro quadrático médio) do nosso modelo de previsão de preços foi de 0.0432 na escala logarítmica, que corresponde a 10^0.0432 ≈ 1.106. Assim, o erro médio do modelo é de aproximadamente 10.6%.\n",
    "\n",
    "A margem de erro de aproximadamente 10% apresentada pelo nosso modelo de previsão de preços imobiliários pode ser considerada bastante satisfatória quando analisamos o contexto completo do mercado. A precificação de imóveis é reconhecidamente um dos desafios mais complexos no campo de modelagem preditiva, pois envolve uma multiplicidade de variáveis que influenciam o valor final de uma propriedade.\n",
    "\n",
    "Os fatores que afetam o preço de um imóvel vão desde aspectos quantitativos facilmente mensuráveis, como localização geográfica, metragem e número de cômodos, até elementos qualitativos mais subjetivos, como estado de conservação, qualidade dos acabamentos e apelo estético. Além disso, existem variáveis macroeconômicas que impactam diretamente o mercado, como taxas de juros, tendências locais e ciclos econômicos.\n",
    "\n",
    "Modelos tradicionais costumam apresentar erros na faixa de 15-20%, nossa solução mantém a margem de erro em torno de 10%. É importante notar que mesmo especialistas humanos frequentemente divergem em suas avaliações, demonstrando a complexidade inerente à tarefa de precificação imobiliária.\n",
    "\n",
    "Em suma, considerando a natureza complexa do mercado imobiliário, a presença de variáveis subjetivas difíceis de quantificar e a volatilidade inerente ao setor, uma margem de erro de 10% demonstra um desempenho adequado para aplicações práticas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
